{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, datasets, Sequential, losses, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集维度为：(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\n",
    "print(f\"训练集维度为：{x_train.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MINST 数据集加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据的前处理模块\n",
    "def preprocess(x, y):\n",
    "    x = tf.cast(x, dtype=tf.float32) / 255.\n",
    "    y = tf.cast(y, dtype=tf.int32)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分子集的个数\n",
    "batchsz = 128\n",
    "train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_db = train_db.map(preprocess).shuffle(10000).batch(batchsz)\n",
    "test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_db = test_db.map(preprocess).batch(batchsz)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/16.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    layers.Conv2D(6, kernel_size=3, strides=1),\n",
    "    layers.MaxPool2D(pool_size=2, strides=2),\n",
    "    layers.ReLU(),\n",
    "    \n",
    "    layers.Conv2D(16, kernel_size=3, strides=1),\n",
    "    layers.MaxPool2D(pool_size=2, strides=2),\n",
    "    layers.ReLU(),\n",
    "\n",
    "    # 打平层\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(120, activation='relu'),\n",
    "    layers.Dense(84, activation='relu'),\n",
    "    layers.Dense(10)\n",
    "])\n",
    "model.build(input_shape=(None, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9702034157871958\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 26, 26, 6)         60        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 6)         0         \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 13, 13, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 11, 11, 16)        880       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 5, 5, 16)          0         \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 5, 5, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 120)               48120     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 60,074\n",
      "Trainable params: 60,074\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 参数统计,约6万个参数,绝大部分为全连接网络的参数\n",
    "print((48120+10164)/60074)\n",
    "model.summary()\n",
    "# 输入28*28*1\n",
    "# 3*3*1+1=10，10*6=60\n",
    "# 3*3*6+1=55, 55*16=880\n",
    "# 5*5*16=400\n",
    "# 400*120 + 120 =48120\n",
    "# 120*84+84=10164\n",
    "# 84*10+10 = 850"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对整个样本进行训练\n",
    "def train_epoch(epoch):\n",
    "    \"\"\"\n",
    "    epoch -- 训练的次数\n",
    "    \"\"\"\n",
    "    # optimizer = optimizers.SGD(learning_rate=0.001) #Adam梯度下降\n",
    "    optimizer = optimizers.Adam(learning_rate=0.001) #Adam梯度下降\n",
    "    criteon = losses.CategoricalCrossentropy(from_logits=True)\n",
    "    for step, (x, y) in enumerate(train_db):\n",
    "        # 训练部分\n",
    "        with tf.GradientTape() as tape:\n",
    "            # 增加维度，[b, 28, 28]-> [b, 28, 28, 1]\n",
    "            x = tf.expand_dims(x, axis=3)\n",
    "            # 前向传播 [b, 784] -> [b, 10]\n",
    "            out = model(x)\n",
    "            y_onehot = tf.one_hot(y, depth=10)\n",
    "            # 计算损失函数\n",
    "            loss = tf.reduce_mean(criteon(y_onehot, out))\n",
    "        # 优化更新参数\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        # w' = w - lr * grad\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(epoch, step, \"loss\",loss.numpy())\n",
    "        \n",
    "    # 测试集的精确度\n",
    "    total_correct, total_num = 0, 0\n",
    "    for x,y in test_db:\n",
    "        x = tf.expand_dims(x, axis=3)\n",
    "        out = model(x)\n",
    "        prob = tf.nn.softmax(out, axis=1) \n",
    "        pred = tf.argmax(prob, axis=1)\n",
    "        pred = tf.cast(pred, dtype=tf.int32)\n",
    "\n",
    "        correct = tf.equal(pred, y)\n",
    "        # 统计正确的个数\n",
    "        correct = tf.reduce_sum(tf.cast(correct, dtype=tf.int32))\n",
    "        total_correct += int(correct)\n",
    "        # 统计样本总数\n",
    "        total_num += x.shape[0]\n",
    "\n",
    "    acc = total_correct / total_num\n",
    "    print(f\"准确率为：{acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    train_epoch(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试集的精确度\n",
    "total_correct, total_num = 0, 0\n",
    "for x,y in test_db:\n",
    "    x = tf.expand_dims(x, axis=3)\n",
    "    out = model(x)\n",
    "    prob = tf.nn.softmax(out, axis=1) \n",
    "    pred = tf.argmax(prob, axis=1)\n",
    "    pred = tf.cast(pred, dtype=tf.int32)\n",
    "\n",
    "    correct = tf.equal(pred, y)\n",
    "    # 统计正确的个数\n",
    "    correct = tf.reduce_sum(tf.cast(correct, dtype=tf.int32))\n",
    "    total_correct += int(correct)\n",
    "    # 统计样本总数\n",
    "    total_num += x.shape[0]\n",
    "\n",
    "acc = total_correct / total_num\n",
    "print(f\"准确率为：{acc}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下为训练过程的分解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建损失函数类，实际计算时直接调用类实例\n",
    "# from_logits=True 标志位将 softmax 激活函数实现在损失函数中，无需手动添加损失函数\n",
    "criteon = losses.CategoricalCrossentropy(from_logits=True)\n",
    "# 训练部分\n",
    "with tf.GradientTape() as tape:\n",
    "    # 增加维度，[b, 28, 28]-> [b, 28, 28, 1]\n",
    "    x = tf.expand_dims(x_train, axis=3)\n",
    "    # 前向传播 [b, 784] -> [b, 10]\n",
    "    out = model(x)\n",
    "    y_onehot = tf.one_hot(y_train, depth=10)\n",
    "    # 计算损失函数\n",
    "    loss = criteon(y_onehot, out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获得损失值后，通过 TF 的梯度记录器 tf.GradientTape() 计算损失函数 loss 对网络参数的 para_m 的梯度，并通过 optimizer 对象自动更新网络权值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优化设置\n",
    "# optimizer = optimizers.SGD(learning_rate=0.001) #随机梯度下降\n",
    "optimizer = optimizers.Adam(learning_rate=0.001) #Adam梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_m = model.trainable_variables\n",
    "# 自动计算梯度\n",
    "grads = tape.gradient(loss, para_m)\n",
    "# 自动更新参数\n",
    "optimizer.apply_gradients(zip(grads, para_m))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
