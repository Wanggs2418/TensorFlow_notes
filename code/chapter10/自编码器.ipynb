{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, datasets, Model, Sequential, optimizers, losses\n",
    "import numpy as np\n",
    "from  PIL import Image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fashion MNIST 实践练习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsz = 128\n",
    "shuffles = 10000\n",
    "lr = 1e-3\n",
    "h_dim = 20\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将多张图片合并并保存为一张大图\n",
    "def save_images(imgs, name):\n",
    "    \n",
    "    new_im = Image.new('L', (280, 280))\n",
    "    index = 0\n",
    "    for i in range(0, 280, 28):\n",
    "        for j in range(0, 280, 28):\n",
    "            im = imgs[index]\n",
    "            im = Image.fromarray(im, mode='L')\n",
    "            new_im.paste(im, (i, j))\n",
    "            index += 1\n",
    "    new_im.save(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#以图片数据为例，适用于全连接层\n",
    "def preprocess(x):\n",
    "    \"\"\"\n",
    "    自定义预处理函数\n",
    "    参数：\n",
    "    x -- 待处理的数据集,维度[b,28,28]\n",
    "    y -- 待处理的数据集,[b]\n",
    "\n",
    "    返回值：\n",
    "    x -- 标准化和扁平化后的 x\n",
    "    y -- 转换为 one_hot 向量\n",
    "    \"\"\"\n",
    "    x = tf.cast(x, dtype=tf.float32) / 255. #标准化0-1\n",
    "    #扁平化-不建议，后续需要用 squeeze函数处理。者此处不设，而在输入的时候对x设置x.reshape(x,(-1, 28*28))\n",
    "    #  注意与 [-1, 28*28]的区别\n",
    "    x = tf.reshape(x, [28*28])\n",
    "\n",
    "\n",
    "    return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自编码器：\n",
    "\n",
    "<img src=\"img/01.jpg\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 编码器：3 层全连接层网络，输出节点为 256、128、20，\n",
    "- 解码器：3 层全连接网络，，输出节点为 128、256、784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集fabshionmnist中x的维度 (60000, 28, 28) 数据集fashionmnist中y的维度 (60000,)\n",
      "<BatchDataset shapes: (None, 784), types: tf.float32> <BatchDataset shapes: (None, 784), types: tf.float32>\n"
     ]
    }
   ],
   "source": [
    "(x,y), (x_test, y_test)=datasets.fashion_mnist.load_data()\n",
    "print('数据集fabshionmnist中x的维度', x.shape, '数据集fashionmnist中y的维度', y.shape)\n",
    "#数据加载后，需要转换为 Dataset对象\n",
    "train_db = tf.data.Dataset.from_tensor_slices(x)\n",
    "test_db = tf.data.Dataset.from_tensor_slices(x_test)\n",
    "\n",
    "# db = db.step1().step2().step3()\n",
    "# shuffle(缓冲区大小，一般是一个较大的常数)，btach(一批的样本数量)\n",
    "train_db = train_db.map(preprocess).shuffle(shuffles).batch(batchsz)\n",
    "test_db = test_db.map(preprocess).batch(batchsz)\n",
    "print(train_db, test_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编码器\n",
    "class autoCode(Model):\n",
    "    def __init__(self):\n",
    "        super(autoCode, self).__init__()\n",
    "        # 创建 Encoders 网络\n",
    "        self.encoder = Sequential([\n",
    "            layers.Dense(256, activation=tf.nn.relu),\n",
    "            layers.Dense(128, activation=tf.nn.relu),\n",
    "            layers.Dense(h_dim)\n",
    "        ])\n",
    "        # 创建 Decoders 网络\n",
    "        self.decoder = Sequential([\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dense(256, activation='relu'),\n",
    "            layers.Dense(784, activation='relu')\n",
    "        ])\n",
    "    # 在 call 函数中实现前向传播过程\n",
    "    def call(self, inputs, training=None):\n",
    "        # 编码过程\n",
    "        # [b, 28*28]=>[b, 20],获得隐藏向量 h\n",
    "        h = self.encoder(inputs)\n",
    "        # 重构图片：[b, 20]=>[b, 784(28*28)]\n",
    "        x_bar = self.decoder(h)\n",
    "        return x_bar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "网络训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"auto_code_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_22 (Sequential)   (4, 20)                   236436    \n",
      "_________________________________________________________________\n",
      "sequential_23 (Sequential)   (4, 784)                  237200    \n",
      "=================================================================\n",
      "Total params: 473,636\n",
      "Trainable params: 473,636\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimizer = optimizers.Adam(learning_rate=lr)\n",
    "model = autoCode()\n",
    "model.build(input_shape=[4, 784])\n",
    "# 参数量\n",
    "# Encoder:236 436\n",
    "# Decoder:237 200\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0.6552934646606445\n",
      "0 1 0.6603056788444519\n",
      "0 2 0.6486604809761047\n",
      "0 3 0.6527636647224426\n",
      "0 4 0.653347373008728\n",
      "0 5 0.6527901887893677\n",
      "0 6 0.6567032337188721\n",
      "0 7 0.6521627902984619\n",
      "0 8 0.6601362228393555\n",
      "0 9 0.6544395089149475\n",
      "0 10 0.653978168964386\n",
      "0 11 0.6511164903640747\n",
      "0 12 0.6501554250717163\n",
      "0 13 0.658989429473877\n",
      "0 14 0.6487768888473511\n",
      "0 15 0.6569609642028809\n",
      "0 16 0.6536020040512085\n",
      "0 17 0.6570005416870117\n",
      "0 18 0.6524364948272705\n",
      "0 19 0.658690333366394\n",
      "0 20 0.6544895172119141\n",
      "0 21 0.6537220478057861\n",
      "0 22 0.6541829705238342\n",
      "0 23 0.6582502126693726\n",
      "0 24 0.6529052257537842\n",
      "0 25 0.655946671962738\n",
      "0 26 0.6517962217330933\n",
      "0 27 0.6554372906684875\n",
      "0 28 0.66205233335495\n",
      "0 29 0.6502474546432495\n",
      "0 30 0.6547035574913025\n",
      "0 31 0.6532195806503296\n",
      "0 32 0.6550509929656982\n",
      "0 33 0.6494882106781006\n",
      "0 34 0.6574854254722595\n",
      "0 35 0.6575288772583008\n",
      "0 36 0.6563966274261475\n",
      "0 37 0.6578041911125183\n",
      "0 38 0.6525934338569641\n",
      "0 39 0.6514668464660645\n",
      "0 40 0.6562644839286804\n",
      "0 41 0.6531639099121094\n",
      "0 42 0.6577684879302979\n",
      "0 43 0.6526810526847839\n",
      "0 44 0.6567670106887817\n",
      "0 45 0.6537243127822876\n",
      "0 46 0.6489974856376648\n",
      "0 47 0.6514247059822083\n",
      "0 48 0.6554785370826721\n",
      "0 49 0.6481491327285767\n",
      "0 50 0.658180832862854\n",
      "0 51 0.6490058898925781\n",
      "0 52 0.6555620431900024\n",
      "0 53 0.6572743058204651\n",
      "0 54 0.652701199054718\n",
      "0 55 0.65716552734375\n",
      "0 56 0.6537042260169983\n",
      "0 57 0.6489682197570801\n",
      "0 58 0.6476738452911377\n",
      "0 59 0.6539708375930786\n",
      "0 60 0.6575398445129395\n",
      "0 61 0.6543164849281311\n",
      "0 62 0.6539469957351685\n",
      "0 63 0.6534625291824341\n",
      "0 64 0.6572062969207764\n",
      "0 65 0.6534348726272583\n",
      "0 66 0.65957111120224\n",
      "0 67 0.6609234809875488\n",
      "0 68 0.6585862040519714\n",
      "0 69 0.6566307544708252\n",
      "0 70 0.6605027914047241\n",
      "0 71 0.6566140055656433\n",
      "0 72 0.6568511724472046\n",
      "0 73 0.6507807374000549\n",
      "0 74 0.6595263481140137\n",
      "0 75 0.6551940441131592\n",
      "0 76 0.6540507674217224\n",
      "0 77 0.6583465337753296\n",
      "0 78 0.6490963697433472\n",
      "0 79 0.6569485664367676\n",
      "0 80 0.6571782231330872\n",
      "0 81 0.654056191444397\n",
      "0 82 0.6599701046943665\n",
      "0 83 0.6485463380813599\n",
      "0 84 0.6527152061462402\n",
      "0 85 0.6555314660072327\n",
      "0 86 0.662237286567688\n",
      "0 87 0.6545863151550293\n",
      "0 88 0.6579111814498901\n",
      "0 89 0.6575547456741333\n",
      "0 90 0.6548460721969604\n",
      "0 91 0.6467705965042114\n",
      "0 92 0.6566934585571289\n",
      "0 93 0.653599202632904\n",
      "0 94 0.6556670665740967\n",
      "0 95 0.6512242555618286\n",
      "0 96 0.6542751789093018\n",
      "0 97 0.6567544937133789\n",
      "0 98 0.6584924459457397\n",
      "0 99 0.6539863348007202\n",
      "0 100 0.654536783695221\n",
      "0 101 0.6620259881019592\n",
      "0 102 0.651995062828064\n",
      "0 103 0.6571832895278931\n",
      "0 104 0.6596790552139282\n",
      "0 105 0.6574808955192566\n",
      "0 106 0.6545897722244263\n",
      "0 107 0.6500254273414612\n",
      "0 108 0.6510002017021179\n",
      "0 109 0.6589252948760986\n",
      "0 110 0.6454818248748779\n",
      "0 111 0.6592322587966919\n",
      "0 112 0.6524289846420288\n",
      "0 113 0.6487332582473755\n",
      "0 114 0.6580167412757874\n",
      "0 115 0.6493755578994751\n",
      "0 116 0.6574907898902893\n",
      "0 117 0.6517306566238403\n",
      "0 118 0.6588224172592163\n",
      "0 119 0.6517913937568665\n",
      "0 120 0.6606068015098572\n",
      "0 121 0.65570068359375\n",
      "0 122 0.6519713401794434\n",
      "0 123 0.6467357873916626\n",
      "0 124 0.6513531804084778\n",
      "0 125 0.6580846905708313\n",
      "0 126 0.6535345911979675\n",
      "0 127 0.6602325439453125\n",
      "0 128 0.651334285736084\n",
      "0 129 0.6554133296012878\n",
      "0 130 0.6579817533493042\n",
      "0 131 0.651540994644165\n",
      "0 132 0.6534695029258728\n",
      "0 133 0.6547428369522095\n",
      "0 134 0.6611822843551636\n",
      "0 135 0.6564526557922363\n",
      "0 136 0.6592172384262085\n",
      "0 137 0.6526564359664917\n",
      "0 138 0.6575760841369629\n",
      "0 139 0.651713490486145\n",
      "0 140 0.6622797250747681\n",
      "0 141 0.6522848010063171\n",
      "0 142 0.6571399569511414\n",
      "0 143 0.6561388373374939\n",
      "0 144 0.6621509194374084\n",
      "0 145 0.6541080474853516\n",
      "0 146 0.6586700081825256\n",
      "0 147 0.6554224491119385\n",
      "0 148 0.6529128551483154\n",
      "0 149 0.658981442451477\n",
      "0 150 0.6595495939254761\n",
      "0 151 0.6519474983215332\n",
      "0 152 0.6527680158615112\n",
      "0 153 0.6499389410018921\n",
      "0 154 0.6554019451141357\n",
      "0 155 0.6543499231338501\n",
      "0 156 0.6522889733314514\n",
      "0 157 0.657731294631958\n",
      "0 158 0.6551903486251831\n",
      "0 159 0.6563011407852173\n",
      "0 160 0.6590226888656616\n",
      "0 161 0.6527901887893677\n",
      "0 162 0.6602779626846313\n",
      "0 163 0.6558417081832886\n",
      "0 164 0.6605390906333923\n",
      "0 165 0.6541028022766113\n",
      "0 166 0.6602792739868164\n",
      "0 167 0.6605823636054993\n",
      "0 168 0.6567589640617371\n",
      "0 169 0.6567292213439941\n",
      "0 170 0.6586906909942627\n",
      "0 171 0.6519918441772461\n",
      "0 172 0.661138117313385\n",
      "0 173 0.6528007984161377\n",
      "0 174 0.6561868786811829\n",
      "0 175 0.6576250791549683\n",
      "0 176 0.6530066728591919\n",
      "0 177 0.6459264159202576\n",
      "0 178 0.6630762815475464\n",
      "0 179 0.6525533199310303\n",
      "0 180 0.6572696566581726\n",
      "0 181 0.6532334685325623\n",
      "0 182 0.6525622606277466\n",
      "0 183 0.6566405296325684\n",
      "0 184 0.6604329347610474\n",
      "0 185 0.6579092741012573\n",
      "0 186 0.6527500748634338\n",
      "0 187 0.6519824266433716\n",
      "0 188 0.6546545624732971\n",
      "0 189 0.6576972603797913\n",
      "0 190 0.6500118970870972\n",
      "0 191 0.656577467918396\n",
      "0 192 0.6569604873657227\n",
      "0 193 0.654198408126831\n",
      "0 194 0.6644961833953857\n",
      "0 195 0.6590826511383057\n",
      "0 196 0.6491605043411255\n",
      "0 197 0.6526879072189331\n",
      "0 198 0.6556093692779541\n",
      "0 199 0.651039719581604\n",
      "0 200 0.6526404619216919\n",
      "0 201 0.6529579162597656\n",
      "0 202 0.6576119661331177\n",
      "0 203 0.6541687846183777\n",
      "0 204 0.6537631750106812\n",
      "0 205 0.6528833508491516\n",
      "0 206 0.6557267904281616\n",
      "0 207 0.6572261452674866\n",
      "0 208 0.6536582112312317\n",
      "0 209 0.6528094410896301\n",
      "0 210 0.650397777557373\n",
      "0 211 0.6568357348442078\n",
      "0 212 0.647395133972168\n",
      "0 213 0.6527003049850464\n",
      "0 214 0.6574879884719849\n",
      "0 215 0.6603177785873413\n",
      "0 216 0.662825345993042\n",
      "0 217 0.657819390296936\n",
      "0 218 0.6556406617164612\n",
      "0 219 0.6519674062728882\n",
      "0 220 0.655268669128418\n",
      "0 221 0.6520159244537354\n",
      "0 222 0.6492384076118469\n",
      "0 223 0.6575380563735962\n",
      "0 224 0.6536425948143005\n",
      "0 225 0.6541609764099121\n",
      "0 226 0.6532180309295654\n",
      "0 227 0.6573739051818848\n",
      "0 228 0.6585820913314819\n",
      "0 229 0.6525525450706482\n",
      "0 230 0.6533300876617432\n",
      "0 231 0.6600245237350464\n",
      "0 232 0.6568899750709534\n",
      "0 233 0.6523535847663879\n",
      "0 234 0.6517706513404846\n",
      "0 235 0.652320921421051\n",
      "0 236 0.6555019617080688\n",
      "0 237 0.6498285531997681\n",
      "0 238 0.6549752950668335\n",
      "0 239 0.6600640416145325\n",
      "0 240 0.655095100402832\n",
      "0 241 0.658146858215332\n",
      "0 242 0.6470694541931152\n",
      "0 243 0.6574238538742065\n",
      "0 244 0.6569196581840515\n",
      "0 245 0.6551682949066162\n",
      "0 246 0.6539944410324097\n",
      "0 247 0.65697181224823\n",
      "0 248 0.652330756187439\n",
      "0 249 0.6540718674659729\n",
      "0 250 0.6538907885551453\n",
      "0 251 0.6567333936691284\n",
      "0 252 0.659247100353241\n",
      "0 253 0.6553841233253479\n",
      "0 254 0.6501736640930176\n",
      "0 255 0.6557140350341797\n",
      "0 256 0.6548396348953247\n",
      "0 257 0.6471085548400879\n",
      "0 258 0.654310941696167\n",
      "0 259 0.6550381183624268\n",
      "0 260 0.6552299857139587\n",
      "0 261 0.6535696983337402\n",
      "0 262 0.6514884233474731\n",
      "0 263 0.6538964509963989\n",
      "0 264 0.6505173444747925\n",
      "0 265 0.6568875312805176\n",
      "0 266 0.6572343111038208\n",
      "0 267 0.6604474782943726\n",
      "0 268 0.6519290804862976\n",
      "0 269 0.6474730968475342\n",
      "0 270 0.6556880474090576\n",
      "0 271 0.6516174077987671\n",
      "0 272 0.6565879583358765\n",
      "0 273 0.6494375467300415\n",
      "0 274 0.6576316356658936\n",
      "0 275 0.6559840440750122\n",
      "0 276 0.6544321775436401\n",
      "0 277 0.6510248184204102\n",
      "0 278 0.6528794765472412\n",
      "0 279 0.6616699695587158\n",
      "0 280 0.6554327011108398\n",
      "0 281 0.6598080396652222\n",
      "0 282 0.657532274723053\n",
      "0 283 0.6592650413513184\n",
      "0 284 0.6576905250549316\n",
      "0 285 0.6511094570159912\n",
      "0 286 0.6540058851242065\n",
      "0 287 0.6551191210746765\n",
      "0 288 0.6622594594955444\n",
      "0 289 0.6569074392318726\n",
      "0 290 0.657198429107666\n",
      "0 291 0.6607245802879333\n",
      "0 292 0.6599777340888977\n",
      "0 293 0.6579145193099976\n",
      "0 294 0.656137228012085\n",
      "0 295 0.6556809544563293\n",
      "0 296 0.6522976756095886\n",
      "0 297 0.6512366533279419\n",
      "0 298 0.6569362878799438\n",
      "0 299 0.6542395353317261\n",
      "0 300 0.6579406261444092\n",
      "0 301 0.6630665063858032\n",
      "0 302 0.6528775691986084\n",
      "0 303 0.6487140655517578\n",
      "0 304 0.6561074256896973\n",
      "0 305 0.6513829231262207\n",
      "0 306 0.65535968542099\n",
      "0 307 0.6617348790168762\n",
      "0 308 0.6526912450790405\n",
      "0 309 0.6523742079734802\n",
      "0 310 0.657619833946228\n",
      "0 311 0.6593633890151978\n",
      "0 312 0.6560157537460327\n",
      "0 313 0.6522649526596069\n",
      "0 314 0.6529495120048523\n",
      "0 315 0.6564608812332153\n",
      "0 316 0.6524769067764282\n",
      "0 317 0.6530601978302002\n",
      "0 318 0.651826024055481\n",
      "0 319 0.6517883539199829\n",
      "0 320 0.6486974954605103\n",
      "0 321 0.65763258934021\n",
      "0 322 0.6522841453552246\n",
      "0 323 0.6500977277755737\n",
      "0 324 0.6595958471298218\n",
      "0 325 0.6615419983863831\n",
      "0 326 0.6573798656463623\n",
      "0 327 0.6589723825454712\n",
      "0 328 0.6539751887321472\n",
      "0 329 0.6524068117141724\n",
      "0 330 0.654754638671875\n",
      "0 331 0.6564286947250366\n",
      "0 332 0.6557300686836243\n",
      "0 333 0.6541934013366699\n",
      "0 334 0.6552263498306274\n",
      "0 335 0.6557864546775818\n",
      "0 336 0.6566507816314697\n",
      "0 337 0.6548115611076355\n",
      "0 338 0.6562743186950684\n",
      "0 339 0.6586869955062866\n",
      "0 340 0.6576285362243652\n",
      "0 341 0.6594047546386719\n",
      "0 342 0.6580028533935547\n",
      "0 343 0.6510115265846252\n",
      "0 344 0.6590270400047302\n",
      "0 345 0.656054675579071\n",
      "0 346 0.6601084470748901\n",
      "0 347 0.6533069610595703\n",
      "0 348 0.6552243828773499\n",
      "0 349 0.6516505479812622\n",
      "0 350 0.6557159423828125\n",
      "0 351 0.6544098854064941\n",
      "0 352 0.6595308780670166\n",
      "0 353 0.6521762609481812\n",
      "0 354 0.6530925035476685\n",
      "0 355 0.6574807167053223\n",
      "0 356 0.6559514403343201\n",
      "0 357 0.6538132429122925\n",
      "0 358 0.6551284790039062\n",
      "0 359 0.6534532308578491\n",
      "0 360 0.6509235501289368\n",
      "0 361 0.6523638963699341\n",
      "0 362 0.6491489410400391\n",
      "0 363 0.6549777984619141\n",
      "0 364 0.6554206609725952\n",
      "0 365 0.6560460329055786\n",
      "0 366 0.6565649509429932\n",
      "0 367 0.6519349217414856\n",
      "0 368 0.6576995849609375\n",
      "0 369 0.6585320234298706\n",
      "0 370 0.6584498286247253\n",
      "0 371 0.6577993631362915\n",
      "0 372 0.6541924476623535\n",
      "0 373 0.6560553312301636\n",
      "0 374 0.6541550159454346\n",
      "0 375 0.6598224639892578\n",
      "0 376 0.6574698686599731\n",
      "0 377 0.6502009034156799\n",
      "0 378 0.6575593948364258\n",
      "0 379 0.6551280617713928\n",
      "0 380 0.6548143625259399\n",
      "0 381 0.6588771939277649\n",
      "0 382 0.6511240005493164\n",
      "0 383 0.6497645378112793\n",
      "0 384 0.6555072069168091\n",
      "0 385 0.6551939249038696\n",
      "0 386 0.6531856060028076\n",
      "0 387 0.6544577479362488\n",
      "0 388 0.6516501307487488\n",
      "0 389 0.6543887853622437\n",
      "0 390 0.6584823131561279\n",
      "0 391 0.6560183763504028\n",
      "0 392 0.6549403667449951\n",
      "0 393 0.6515136957168579\n",
      "0 394 0.6577412486076355\n",
      "0 395 0.6545260548591614\n",
      "0 396 0.6570257544517517\n",
      "0 397 0.6572262644767761\n",
      "0 398 0.6551181077957153\n",
      "0 399 0.6528038382530212\n",
      "0 400 0.6527042388916016\n",
      "0 401 0.6517049074172974\n",
      "0 402 0.6506727933883667\n",
      "0 403 0.658253014087677\n",
      "0 404 0.6565372943878174\n",
      "0 405 0.663686990737915\n",
      "0 406 0.6495202779769897\n",
      "0 407 0.6555485725402832\n",
      "0 408 0.6520488858222961\n",
      "0 409 0.6558486223220825\n",
      "0 410 0.653976559638977\n",
      "0 411 0.6533686518669128\n",
      "0 412 0.6581698060035706\n",
      "0 413 0.6562063694000244\n",
      "0 414 0.6558085680007935\n",
      "0 415 0.6530739068984985\n",
      "0 416 0.6492825150489807\n",
      "0 417 0.6540235877037048\n",
      "0 418 0.6537428498268127\n",
      "0 419 0.6542853116989136\n",
      "0 420 0.6551448106765747\n",
      "0 421 0.6566578149795532\n",
      "0 422 0.6539708375930786\n",
      "0 423 0.6511804461479187\n",
      "0 424 0.6482952833175659\n",
      "0 425 0.6583859920501709\n",
      "0 426 0.6551077961921692\n",
      "0 427 0.6630382537841797\n",
      "0 428 0.6496860980987549\n",
      "0 429 0.6479710340499878\n",
      "0 430 0.6570994853973389\n",
      "0 431 0.6534932851791382\n",
      "0 432 0.6521506309509277\n",
      "0 433 0.6493352651596069\n",
      "0 434 0.6557491421699524\n",
      "0 435 0.6506022214889526\n",
      "0 436 0.6554624438285828\n",
      "0 437 0.6523292064666748\n",
      "0 438 0.6534865498542786\n",
      "0 439 0.654609203338623\n",
      "0 440 0.6595607995986938\n",
      "0 441 0.6555553078651428\n",
      "0 442 0.652305543422699\n",
      "0 443 0.6578103303909302\n",
      "0 444 0.6604425311088562\n",
      "0 445 0.6545404195785522\n",
      "0 446 0.6484348177909851\n",
      "0 447 0.649574875831604\n",
      "0 448 0.6547442674636841\n",
      "0 449 0.6520440578460693\n",
      "0 450 0.6578812599182129\n",
      "0 451 0.65477055311203\n",
      "0 452 0.6600481867790222\n",
      "0 453 0.6527509689331055\n",
      "0 454 0.6557351350784302\n",
      "0 455 0.653252363204956\n",
      "0 456 0.6558785438537598\n",
      "0 457 0.6588910818099976\n",
      "0 458 0.6568435430526733\n",
      "0 459 0.6607353091239929\n",
      "0 460 0.6616882085800171\n",
      "0 461 0.6566804647445679\n",
      "0 462 0.6525428295135498\n",
      "0 463 0.6473445892333984\n",
      "0 464 0.6562449932098389\n",
      "0 465 0.6531246900558472\n",
      "0 466 0.6573894023895264\n",
      "0 467 0.6490778923034668\n",
      "0 468 0.6527338624000549\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\TensorFlow_notes\\code\\chapter10\\自编码器.ipynb 单元格 12\u001b[0m in \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/TensorFlow_notes/code/chapter10/%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m step, x \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_db):\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/TensorFlow_notes/code/chapter10/%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/TensorFlow_notes/code/chapter10/%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.ipynb#X23sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         logits \u001b[39m=\u001b[39m model(x)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/TensorFlow_notes/code/chapter10/%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.ipynb#X23sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         loss \u001b[39m=\u001b[39m losses\u001b[39m.\u001b[39mbinary_crossentropy(x, logits, from_logits\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/TensorFlow_notes/code/chapter10/%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         loss \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreduce_mean(loss)\n",
      "File \u001b[1;32me:\\pythonwork\\Jupyter\\tensorflowenv38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1012\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1008\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_build(inputs)\n\u001b[0;32m   1010\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1011\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object):\n\u001b[1;32m-> 1012\u001b[0m   outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1014\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1015\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "\u001b[1;32me:\\TensorFlow_notes\\code\\chapter10\\自编码器.ipynb 单元格 12\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/TensorFlow_notes/code/chapter10/%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.ipynb#X23sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(inputs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/TensorFlow_notes/code/chapter10/%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.ipynb#X23sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# 重构图片：[b, 20]=>[b, 784(28*28)]\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/TensorFlow_notes/code/chapter10/%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.ipynb#X23sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m x_bar \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(h)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/TensorFlow_notes/code/chapter10/%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.ipynb#X23sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x_bar\n",
      "File \u001b[1;32me:\\pythonwork\\Jupyter\\tensorflowenv38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1012\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1008\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_build(inputs)\n\u001b[0;32m   1010\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1011\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object):\n\u001b[1;32m-> 1012\u001b[0m   outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1014\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1015\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32me:\\pythonwork\\Jupyter\\tensorflowenv38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:375\u001b[0m, in \u001b[0;36mSequential.call\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    373\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilt:\n\u001b[0;32m    374\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_graph_network(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutputs)\n\u001b[1;32m--> 375\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(Sequential, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mcall(inputs, training\u001b[39m=\u001b[39;49mtraining, mask\u001b[39m=\u001b[39;49mmask)\n\u001b[0;32m    377\u001b[0m outputs \u001b[39m=\u001b[39m inputs  \u001b[39m# handle the corner case where self.layers is empty\u001b[39;00m\n\u001b[0;32m    378\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m    379\u001b[0m   \u001b[39m# During each iteration, `inputs` are the inputs to `layer`, and `outputs`\u001b[39;00m\n\u001b[0;32m    380\u001b[0m   \u001b[39m# are the outputs of `layer` applied to `inputs`. At the end of each\u001b[39;00m\n\u001b[0;32m    381\u001b[0m   \u001b[39m# iteration `inputs` is set to `outputs` to prepare for the next layer.\u001b[39;00m\n",
      "File \u001b[1;32me:\\pythonwork\\Jupyter\\tensorflowenv38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:424\u001b[0m, in \u001b[0;36mFunctional.call\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    406\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, inputs, training\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    407\u001b[0m   \u001b[39m\"\"\"Calls the model on new inputs.\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \n\u001b[0;32m    409\u001b[0m \u001b[39m  In this case `call` just reapplies\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[39m      a list of tensors if there are more than one outputs.\u001b[39;00m\n\u001b[0;32m    423\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 424\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_internal_graph(\n\u001b[0;32m    425\u001b[0m       inputs, training\u001b[39m=\u001b[39;49mtraining, mask\u001b[39m=\u001b[39;49mmask)\n",
      "File \u001b[1;32me:\\pythonwork\\Jupyter\\tensorflowenv38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:560\u001b[0m, in \u001b[0;36mFunctional._run_internal_graph\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    557\u001b[0m   \u001b[39mcontinue\u001b[39;00m  \u001b[39m# Node is not computable, try skipping.\u001b[39;00m\n\u001b[0;32m    559\u001b[0m args, kwargs \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39mmap_arguments(tensor_dict)\n\u001b[1;32m--> 560\u001b[0m outputs \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39;49mlayer(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    562\u001b[0m \u001b[39m# Update tensor_dict.\u001b[39;00m\n\u001b[0;32m    563\u001b[0m \u001b[39mfor\u001b[39;00m x_id, y \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(node\u001b[39m.\u001b[39mflat_output_ids, nest\u001b[39m.\u001b[39mflatten(outputs)):\n",
      "File \u001b[1;32me:\\pythonwork\\Jupyter\\tensorflowenv38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1012\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1008\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_build(inputs)\n\u001b[0;32m   1010\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1011\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object):\n\u001b[1;32m-> 1012\u001b[0m   outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1014\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1015\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32me:\\pythonwork\\Jupyter\\tensorflowenv38\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:1207\u001b[0m, in \u001b[0;36mDense.call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   1206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, inputs):\n\u001b[1;32m-> 1207\u001b[0m   \u001b[39mreturn\u001b[39;00m core_ops\u001b[39m.\u001b[39;49mdense(\n\u001b[0;32m   1208\u001b[0m       inputs,\n\u001b[0;32m   1209\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel,\n\u001b[0;32m   1210\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[0;32m   1211\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactivation,\n\u001b[0;32m   1212\u001b[0m       dtype\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute_dtype_object)\n",
      "File \u001b[1;32me:\\pythonwork\\Jupyter\\tensorflowenv38\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\ops\\core.py:53\u001b[0m, in \u001b[0;36mdense\u001b[1;34m(inputs, kernel, bias, activation, dtype)\u001b[0m\n\u001b[0;32m     51\u001b[0m     outputs \u001b[39m=\u001b[39m sparse_ops\u001b[39m.\u001b[39msparse_tensor_dense_matmul(inputs, kernel)\n\u001b[0;32m     52\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 53\u001b[0m     outputs \u001b[39m=\u001b[39m gen_math_ops\u001b[39m.\u001b[39;49mmat_mul(inputs, kernel)\n\u001b[0;32m     54\u001b[0m \u001b[39m# Broadcast kernel to inputs.\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     56\u001b[0m   outputs \u001b[39m=\u001b[39m standard_ops\u001b[39m.\u001b[39mtensordot(inputs, kernel, [[rank \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m], [\u001b[39m0\u001b[39m]])\n",
      "File \u001b[1;32me:\\pythonwork\\Jupyter\\tensorflowenv38\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:5526\u001b[0m, in \u001b[0;36mmat_mul\u001b[1;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[0;32m   5524\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m   5525\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 5526\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m   5527\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mMatMul\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, a, b, \u001b[39m\"\u001b[39;49m\u001b[39mtranspose_a\u001b[39;49m\u001b[39m\"\u001b[39;49m, transpose_a, \u001b[39m\"\u001b[39;49m\u001b[39mtranspose_b\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   5528\u001b[0m       transpose_b)\n\u001b[0;32m   5529\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   5530\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for step, x in enumerate(train_db):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x)\n",
    "            loss = losses.binary_crossentropy(x, logits, from_logits=True)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        if epoch % 10 ==0 and step % 100 == 0:\n",
    "            print(epoch, step, float(loss))\n",
    "\n",
    "        # evaluation\n",
    "        x = next(iter(test_db))\n",
    "        x_rel = tf.reshape(x, [-1, 28, 28])\n",
    "        # print(x.shape)\n",
    "        logits = model(x)\n",
    "        x_bar = tf.sigmoid(logits)\n",
    "        # [b, 28*28]=>[b, 28, 28]\n",
    "        x_bar = tf.reshape(x_bar, [-1, 28, 28])\n",
    "\n",
    "        # [b, 28, 28]=>[2b, 28, 28]\n",
    "        x_concat = tf.concat([x_rel[:50], x_bar[:50]], axis=0)\n",
    "        # x_concat = x_bar\n",
    "        x_concat = x_concat.numpy() * 255.\n",
    "        x_concat = x_concat.astype(np.uint8)\n",
    "        save_images(x_concat, 'autoCode/epoch_%d.png'%epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = autoCode()\n",
    "model.compile(optimizer=optimizer, loss=losses.categorical_crossentropy, metrics=['accuracy'])\n",
    "history = model.fit(train_db, epochs=epochs, validation_data=test_db)\n",
    "model.evaluate(test_db)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
